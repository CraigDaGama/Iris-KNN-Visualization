# ðŸŒ¸ Iris Dataset â€“ ML Classification & Visualization

This repository is a detailed, beginner-friendly walkthrough of the classic **Iris dataset**, showcasing how different machine learning models work and how visualization helps interpret them.

## ðŸ“¦ Whatâ€™s Inside

- ðŸ” **Exploratory Data Analysis (EDA)** â€“ Histograms, boxplots, heatmaps, pairplots
- ðŸ“Š **Distance-Based Classifiers** â€“ KNN with Euclidean, Manhattan, Chebyshev, Minkowski
- ðŸ” **Model Comparisons** â€“ Logistic Regression and SVM
- ðŸŽ¨ **Visual Insights** â€“ PCA, t-SNE, decision boundaries, Andrews curves, Parallel Coordinates
- âš™ï¸ **Metric Evaluation** â€“ Accuracy, precision, recall, F1-score, classification reports
- ðŸ“‰ **Decision Boundary Visualization** â€“ Class separability in 2D
- ðŸŽ›ï¸ **Interactive Components** â€“ k-value tuning with ipywidgets (planned)

---

## ðŸ“š Techniques Used & Why

### âœ… K-Nearest Neighbors (KNN)
- **How it works**: Classifies based on the majority class among â€˜kâ€™ closest neighbors.
- **Distance metrics used**:
  - **Euclidean**: Default, straight-line distance.
  - **Manhattan**: Useful when axis-aligned moves are logical.
  - **Chebyshev**: Picks max difference between features.
- **Why used here**: Great for small datasets like Iris; intuitive to understand.

### âœ… Logistic Regression
- **Why**: Baseline model, interpretable and works well for linear separation.
- **Works by**: Estimating probabilities using a sigmoid function.

### âœ… Support Vector Machine (SVM)
- **Why**: Powerful for smaller, well-separated datasets.
- **Works by**: Finding a hyperplane that best separates classes.

---

## ðŸ“Š Visualizations & What They Show

### ðŸ“Œ Pairplot
- Shows relationships between each pair of features.
- Highlights class separation visually.

### ðŸ“Œ Heatmap
- Correlation matrix to find how features relate.

### ðŸ“Œ Boxplot
- Shows distribution and potential outliers across classes.

### ðŸ“Œ PCA Plot
- Reduces dimensions to 2 while preserving variance.
- Great for plotting complex data simply.

### ðŸ“Œ t-SNE
- Nonlinear dimensionality reduction; visualizes true clusters.
- Useful when PCA doesn't clearly separate classes.

### ðŸ“Œ Decision Boundaries
- Shows how each classifier splits feature space.
- Only 2 features used (Sepal Length and Petal Length) for 2D clarity.

### ðŸ“Œ Parallel Coordinates & Andrews Curves
- High-dimensional visualizations showing patterns and separability between classes.

---

## ðŸ“ˆ Evaluation

Each classifier is evaluated using:
- âœ… Accuracy
- âœ… Precision, Recall, F1-score (per class)
- âœ… Confusion matrix-like visualizations via count plots

---

## ðŸ§  Learning Goals

This notebook aims to help beginners understand:
- How KNN and other classifiers work.
- When and why different distance metrics matter.
- How to interpret plots and extract meaning from them.
- Practical implementation of classifiers in scikit-learn.
- How visualizations make your model explainable.

---

## ðŸ”§ Requirements

- Python 3.x
- Libraries:
  - `numpy`
  - `pandas`
  - `matplotlib`
  - `seaborn`
  - `scikit-learn`

---

## ðŸš€ Run it on Google Colab

> Copy this repo and open the notebook in [Google Colab](https://colab.research.google.com/) to explore interactively.

---

## ðŸ“¬ Feedback & Contributions

Open an issue if you find bugs or want more visualizations or classifiers added!

---

## ðŸ“œ License

This project is under the MIT License. Feel free to use and modify.

